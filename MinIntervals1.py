import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import io
from datetime import datetime
import math
from typing import Tuple, List, Dict
import re

# Configuration de la page
st.set_page_config(
    page_title="Analyseur d'Intervalles Min√©ralis√©s",
    page_icon="‚õèÔ∏è",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS personnalis√©
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #1e40af, #3b82f6);
        padding: 1.5rem;
        border-radius: 12px;
        color: white;
        margin-bottom: 2rem;
        text-align: center;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
    }
    .metric-card {
        background: white;
        padding: 1.5rem;
        border-radius: 8px;
        border-left: 4px solid #3b82f6;
        box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        margin: 1rem 0;
    }
    .success-box {
        background: #dcfce7;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #16a34a;
        margin: 1rem 0;
    }
    .warning-box {
        background: #fef3c7;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #f59e0b;
        margin: 1rem 0;
    }
    .error-box {
        background: #fef2f2;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #ef4444;
        margin: 1rem 0;
    }
    .info-box {
        background: #dbeafe;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #3b82f6;
        margin: 1rem 0;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 4px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 60px;
        padding-left: 24px;
        padding-right: 24px;
        background-color: #f8fafc;
        border-radius: 8px 8px 0 0;
    }
    .stTabs [aria-selected="true"] {
        background-color: #3b82f6;
        color: white;
    }
</style>
""", unsafe_allow_html=True)

# ===============================================================================
# FONCTIONS DE CALCUL G√âOLOGIQUE
# ===============================================================================

@st.cache_data
def calculate_distance_to_shear(easting: float, northing: float, elevation: float) -> float:
    """
    Calcule la distance perpendiculaire d'un point √† la shear zone planaire
    
    Param√®tres shear zone:
    - Strike: N45¬∞E (azimut 045¬∞)
    - Dip: 75¬∞ vers SE
    - Centre: E450000, N5550000, Z150m
    """
    # Param√®tres de la shear zone
    center_easting = 450000
    center_northing = 5550000
    center_depth = 100  # Profondeur du centre
    strike_azimuth = 45  # Degr√©s
    dip_angle = 75  # Degr√©s
    
    # Conversion en radians
    strike_rad = math.radians(strike_azimuth)
    dip_rad = math.radians(dip_angle)
    
    # Vecteur normal au plan de la shear zone
    normal_x = math.cos(strike_rad) * math.sin(dip_rad)
    normal_y = -math.sin(strike_rad) * math.sin(dip_rad)
    normal_z = math.cos(dip_rad)
    
    # Point de r√©f√©rence sur le plan
    ref_x = center_easting
    ref_y = center_northing
    ref_z = 250 - center_depth  # Convention: √©l√©vation = 250 - profondeur
    
    # Vecteur du point de r√©f√©rence au point test√©
    dx = easting - ref_x
    dy = northing - ref_y
    dz = elevation - ref_z
    
    # Distance perpendiculaire au plan
    distance = abs(dx * normal_x + dy * normal_y + dz * normal_z)
    
    return distance

def generate_grade_from_distance(distance: float) -> float:
    """G√©n√®re une teneur bas√©e sur la distance √† la shear zone"""
    
    # Grade de base tr√®s faible
    base_grade = 0.01 + np.random.random() * 0.05
    
    if distance < 20:  # Zone d'influence
        proximity_factor = max(0, (20 - distance) / 20)
        
        if distance < 5:  # Tr√®s proche: hautes teneurs
            grade = 0.5 + np.random.random() * 3.0 * proximity_factor
            if np.random.random() < 0.2:  # 20% de valeurs exceptionnelles
                grade += np.random.random() * 5
        elif distance < 10:  # Proche: teneurs mod√©r√©es
            grade = 0.2 + np.random.random() * 1.5 * proximity_factor
            if np.random.random() < 0.3:
                grade += np.random.random() * 2
        else:  # Zone d'influence
            grade = 0.1 + np.random.random() * 0.8 * proximity_factor
    else:
        grade = base_grade
    
    # Quelques anomalies dispers√©es (5%)
    if np.random.random() < 0.05:
        grade = 0.3 + np.random.random() * 1.0
    
    return grade

@st.cache_data
def generate_demo_data() -> Tuple[pd.DataFrame, pd.DataFrame]:
    """G√©n√®re des donn√©es de d√©monstration optimis√©es - 100 forages, 5000+ √©chantillons"""
    
    # Param√®tres de la shear zone
    center_easting = 450000
    center_northing = 5550000
    strike_length = 800
    dip_extent = 200
    strike_azimuth = 45
    center_depth = 100
    
    # G√©n√©rer 100 forages orient√©s
    drillholes = []
    for i in range(1, 101):
        # Position optimis√©e pour intercepter la shear zone
        offset_strike = (np.random.random() - 0.5) * strike_length * 0.8
        offset_dip = (np.random.random() - 0.5) * dip_extent * 0.8
        
        strike_rad = math.radians(strike_azimuth)
        
        surface_easting = center_easting + offset_strike * math.sin(strike_rad) + \
                         (np.random.random() - 0.5) * 100
        surface_northing = center_northing + offset_strike * math.cos(strike_rad) + \
                          (np.random.random() - 0.5) * 100
        
        # Azimut optimal pour croiser la shear zone
        optimal_azimuth = (strike_azimuth + 90) % 360
        azimuth_variation = (np.random.random() - 0.5) * 60
        
        drillholes.append({
            'HoleID': f'DDH-{i:03d}',
            'Easting': round(surface_easting, 2),
            'Northing': round(surface_northing, 2),
            'Elevation': round(250 + (np.random.random() - 0.5) * 50, 2),
            'Azimuth': round(optimal_azimuth + azimuth_variation, 1),
            'Dip': round(-60 + (np.random.random() - 0.5) * 20, 1),
            'Depth': round(200 + np.random.random() * 200, 2)
        })
    
    drillholes_df = pd.DataFrame(drillholes)
    
    # G√©n√©rer les √©chantillons
    samples = []
    sample_id = 1
    
    for _, hole in drillholes_df.iterrows():
        depth = 1
        while depth < hole['Depth']:
            # Calculer position 3D de l'√©chantillon
            azimuth_rad = math.radians(hole['Azimuth'])
            dip_rad = math.radians(abs(hole['Dip']))
            
            sample_easting = hole['Easting'] + depth * math.sin(azimuth_rad) * math.cos(dip_rad)
            sample_northing = hole['Northing'] + depth * math.cos(azimuth_rad) * math.cos(dip_rad)
            sample_elevation = hole['Elevation'] - depth * math.sin(dip_rad)
            
            # Calculer distance √† la shear zone
            distance_to_shear = calculate_distance_to_shear(
                sample_easting, sample_northing, sample_elevation
            )
            
            # G√©n√©rer teneur bas√©e sur la proximit√©
            grade = generate_grade_from_distance(distance_to_shear)
            
            # D√©terminer la zone g√©ologique
            if distance_to_shear < 5:
                zone = 'Shear_Zone_Core'
            elif distance_to_shear < 15:
                zone = 'Shear_Zone_Halo'
            else:
                zone = 'Host_Rock'
            
            samples.append({
                'SampleID': f'{hole["HoleID"]}-{sample_id:04d}',
                'HoleID': hole['HoleID'],
                'From': depth,
                'To': depth + 2,
                'Length': 2.0,
                'Au': round(grade, 3),
                'Easting': round(sample_easting, 2),
                'Northing': round(sample_northing, 2),
                'Elevation': round(sample_elevation, 2),
                'DistanceToShear': round(distance_to_shear, 2),
                'Zone': zone
            })
            
            depth += 2
            sample_id += 1
    
    samples_df = pd.DataFrame(samples)
    
    return samples_df, drillholes_df

# ===============================================================================
# FONCTIONS D'ANALYSE
# ===============================================================================

def identify_potential_intervals(hole_samples: pd.DataFrame, config: Dict) -> List[Dict]:
    """Identifie les intervalles potentiels dans un forage"""
    
    cutoff_grade = config['cutoff_grade']
    max_distance = config['max_distance']
    min_length = config['min_length']
    min_samples = config['min_samples']
    
    intervals = []
    current_interval = None
    
    for _, sample in hole_samples.iterrows():
        grade = sample['Au']
        distance = sample.get('DistanceToShear', 999)
        
        # Crit√®res de s√©lection
        meets_grade = grade >= cutoff_grade
        meets_distance = distance <= max_distance
        
        if meets_grade and meets_distance:
            if current_interval is None:
                # Commencer un nouvel intervalle
                current_interval = {
                    'start': sample['From'],
                    'end': sample['To'],
                    'samples': [sample],
                    'length': sample['Length'],
                    'grade_sum': grade,
                    'sample_count': 1,
                    'max_grade': grade,
                    'min_grade': grade,
                    'distance_sum': distance,
                    'max_distance': distance,
                    'min_distance': distance
                }
            else:
                # V√©rifier la continuit√© (gap max 4m)
                gap = sample['From'] - current_interval['end']
                if gap <= 4:
                    # √âtendre l'intervalle
                    current_interval['end'] = sample['To']
                    current_interval['samples'].append(sample)
                    current_interval['length'] += sample['Length']
                    current_interval['grade_sum'] += grade
                    current_interval['sample_count'] += 1
                    current_interval['max_grade'] = max(current_interval['max_grade'], grade)
                    current_interval['min_grade'] = min(current_interval['min_grade'], grade)
                    current_interval['distance_sum'] += distance
                    current_interval['max_distance'] = max(current_interval['max_distance'], distance)
                    current_interval['min_distance'] = min(current_interval['min_distance'], distance)
                else:
                    # Finaliser l'intervalle pr√©c√©dent
                    if (current_interval['length'] >= min_length and 
                        current_interval['sample_count'] >= min_samples):
                        intervals.append(finalize_interval(current_interval))
                    
                    # Commencer un nouvel intervalle
                    current_interval = {
                        'start': sample['From'],
                        'end': sample['To'],
                        'samples': [sample],
                        'length': sample['Length'],
                        'grade_sum': grade,
                        'sample_count': 1,
                        'max_grade': grade,
                        'min_grade': grade,
                        'distance_sum': distance,
                        'max_distance': distance,
                        'min_distance': distance
                    }
        else:
            # Finaliser l'intervalle en cours si valide
            if (current_interval and 
                current_interval['length'] >= min_length and 
                current_interval['sample_count'] >= min_samples):
                intervals.append(finalize_interval(current_interval))
            current_interval = None
    
    # Traiter le dernier intervalle
    if (current_interval and 
        current_interval['length'] >= min_length and 
        current_interval['sample_count'] >= min_samples):
        intervals.append(finalize_interval(current_interval))
    
    return intervals

def finalize_interval(interval_data: Dict) -> Dict:
    """Finalise un intervalle avec calculs des moyennes"""
    
    return {
        'start': interval_data['start'],
        'end': interval_data['end'],
        'length': interval_data['length'],
        'avg_grade': interval_data['grade_sum'] / interval_data['sample_count'],
        'max_grade': interval_data['max_grade'],
        'min_grade': interval_data['min_grade'],
        'sample_count': interval_data['sample_count'],
        'avg_distance': interval_data['distance_sum'] / interval_data['sample_count'],
        'min_distance': interval_data['min_distance'],
        'max_distance': interval_data['max_distance']
    }

def calculate_mineral_intervals(samples_df: pd.DataFrame, config: Dict) -> pd.DataFrame:
    """Calcule les intervalles min√©ralis√©s selon les crit√®res"""
    
    intervals = []
    interval_id = 1
    
    for hole_id, hole_samples in samples_df.groupby('HoleID'):
        hole_samples = hole_samples.sort_values('From')
        
        # Identifier les intervalles potentiels
        potential_intervals = identify_potential_intervals(hole_samples, config)
        
        for interval in potential_intervals:
            intervals.append({
                'IntervalID': interval_id,
                'HoleID': hole_id,
                'From': interval['start'],
                'To': interval['end'],
                'Length': interval['length'],
                'AvgGrade': interval['avg_grade'],
                'MaxGrade': interval['max_grade'],
                'MinGrade': interval['min_grade'],
                'SampleCount': interval['sample_count'],
                'GradeXLength': interval['avg_grade'] * interval['length'],
                'AvgDistance': interval.get('avg_distance', 0),
                'MinDistance': interval.get('min_distance', 0),
                'MaxDistance': interval.get('max_distance', 0),
                'IntervalsBefore': 1,
                'IntervalsAfter': 1,
                'DilutedLength': 0.0,
                'Note': ''
            })
            interval_id += 1
    
    return pd.DataFrame(intervals)

def apply_dilution_constraints(intervals_df: pd.DataFrame, samples_df: pd.DataFrame, 
                             max_dilution: float) -> pd.DataFrame:
    """Applique les contraintes de dilution pour regrouper les intervalles"""
    
    final_intervals = []
    
    # Grouper les intervalles par forage
    for hole_id, hole_intervals in intervals_df.groupby('HoleID'):
        hole_intervals = hole_intervals.sort_values('From')
        
        if len(hole_intervals) == 1:
            # Un seul intervalle: le garder tel quel
            interval = hole_intervals.iloc[0].copy()
            interval['IntervalsBefore'] = 1
            interval['IntervalsAfter'] = 1
            final_intervals.append(interval)
        else:
            # Multiples intervalles: appliquer la logique de regroupement
            merged_interval = apply_merging_logic(
                hole_intervals, samples_df, hole_id, max_dilution
            )
            final_intervals.append(merged_interval)
    
    return pd.DataFrame(final_intervals)

def apply_merging_logic(hole_intervals: pd.DataFrame, samples_df: pd.DataFrame, 
                       hole_id: str, max_dilution: float) -> Dict:
    """Applique la logique de regroupement pour un forage"""
    
    first_interval = hole_intervals.iloc[0]
    last_interval = hole_intervals.iloc[-1]
    
    # Calculer la dilution totale
    total_span = last_interval['To'] - first_interval['From']
    total_mineralized = hole_intervals['Length'].sum()
    dilution = total_span - total_mineralized
    
    intervals_before = len(hole_intervals)
    
    if dilution <= max_dilution:
        # Regrouper avec grade dilu√©
        hole_samples = samples_df[samples_df['HoleID'] == hole_id]
        span_samples = hole_samples[
            (hole_samples['From'] >= first_interval['From']) & 
            (hole_samples['To'] <= last_interval['To'])
        ]
        
        # Calcul du grade dilu√©
        diluted_grade = span_samples['Au'].mean()
        diluted_sample_count = len(span_samples)
        
        return {
            'IntervalID': first_interval['IntervalID'],
            'HoleID': hole_id,
            'From': first_interval['From'],
            'To': last_interval['To'],
            'Length': total_span,
            'AvgGrade': diluted_grade,
            'MaxGrade': hole_intervals['MaxGrade'].max(),
            'MinGrade': hole_intervals['MinGrade'].min(),
            'SampleCount': diluted_sample_count,
            'GradeXLength': diluted_grade * total_span,
            'AvgDistance': hole_intervals['AvgDistance'].mean(),
            'MinDistance': hole_intervals['MinDistance'].min(),
            'MaxDistance': hole_intervals['MaxDistance'].max(),
            'IntervalsBefore': intervals_before,
            'IntervalsAfter': 1,
            'DilutedLength': dilution,
            'Note': f'Regroup√© ({intervals_before} intervalles)'
        }
    else:
        # S√©lectionner le meilleur intervalle
        best_interval = hole_intervals.loc[
            hole_intervals['GradeXLength'].idxmax()
        ].copy()
        
        best_interval['IntervalsBefore'] = intervals_before
        best_interval['IntervalsAfter'] = 1
        best_interval['DilutedLength'] = 0.0
        best_interval['Note'] = f'Meilleur s√©lectionn√© (dilution {dilution:.1f}m > {max_dilution}m)'
        
        return best_interval.to_dict()

# ===============================================================================
# FONCTIONS UTILITAIRES
# ===============================================================================

def load_csv_file(uploaded_file):
    """Charge un fichier CSV upload√©"""
    try:
        df = pd.read_csv(uploaded_file)
        return df
    except Exception as e:
        st.error(f"‚ùå Erreur lors du chargement: {str(e)}")
        return None

def auto_detect_columns(df):
    """D√©tection automatique des colonnes"""
    if df is None or df.empty:
        return {}
    
    columns = df.columns.tolist()
    detected = {}
    
    patterns = {
        'HoleID': r'hole|drill|ddh|bh|sondage',
        'From': r'from|debut|start|de',
        'To': r'to|fin|end|a',
        'Au': r'au|gold|or',
        'Easting': r'east|x|utm_x',
        'Northing': r'north|y|utm_y',
        'Elevation': r'elev|z|alt|prof'
    }
    
    for field, pattern in patterns.items():
        for col in columns:
            if re.search(pattern, col, re.IGNORECASE):
                detected[field] = col
                break
    
    return detected

def validate_data(df, mapping):
    """Validation des donn√©es"""
    errors = []
    
    required_fields = ['HoleID', 'From', 'To', 'Au']
    for field in required_fields:
        if not mapping.get(field):
            errors.append(f"Champ obligatoire manquant: {field}")
    
    if mapping.get('From') and mapping.get('To'):
        try:
            invalid_intervals = df[
                pd.to_numeric(df[mapping['From']], errors='coerce') >= 
                pd.to_numeric(df[mapping['To']], errors='coerce')
            ]
            if len(invalid_intervals) > 0:
                errors.append(f"{len(invalid_intervals)} intervalles invalides (From >= To)")
        except:
            errors.append("Erreur dans les colonnes From/To")
    
    return errors

def calculate_distances_if_needed(df, mapping):
    """Calcule les distances √† la shear zone si les coordonn√©es sont disponibles"""
    
    # V√©rifier si les coordonn√©es sont disponibles
    coord_fields = ['Easting', 'Northing', 'Elevation']
    has_coords = all(mapping.get(field) for field in coord_fields)
    
    if has_coords:
        # Calculer les distances
        st.info("üìç Calcul des distances √† la shear zone en cours...")
        
        progress_bar = st.progress(0)
        distances = []
        
        for i, row in df.iterrows():
            try:
                easting = float(row[mapping['Easting']])
                northing = float(row[mapping['Northing']])
                elevation = float(row[mapping['Elevation']])
                
                distance = calculate_distance_to_shear(easting, northing, elevation)
                distances.append(distance)
                
                # Mise √† jour de la barre de progression
                if i % 100 == 0:
                    progress_bar.progress(min(i / len(df), 1.0))
                    
            except (ValueError, TypeError):
                distances.append(999)  # Distance par d√©faut si erreur
        
        progress_bar.progress(1.0)
        df['DistanceToShear'] = distances
        
        progress_bar.empty()
        st.success(f"‚úÖ Distances calcul√©es pour {len(df)} √©chantillons")
        
        return df, True
    else:
        # Pas de coordonn√©es - utiliser distance par d√©faut
        df['DistanceToShear'] = 999
        st.warning("‚ö†Ô∏è Coordonn√©es manquantes - distance par d√©faut utilis√©e (999m)")
        return df, False

# ===============================================================================
# FONCTIONS DE VISUALISATION
# ===============================================================================

def create_grade_distribution_plot(df, mapping):
    """Cr√©er un graphique de distribution des teneurs"""
    
    if not mapping.get('Au'):
        return None
    
    grades = pd.to_numeric(df[mapping['Au']], errors='coerce').dropna()
    
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=(
            'Distribution des Teneurs Au (g/t)', 
            'Bo√Æte √† Moustaches', 
            'Courbe Cumulative', 
            'Teneurs vs Distance Shear Zone'
        ),
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    # Histogramme
    fig.add_trace(
        go.Histogram(x=grades, nbinsx=50, name='Distribution', opacity=0.7),
        row=1, col=1
    )
    
    # Box plot
    fig.add_trace(
        go.Box(y=grades, name='Box Plot'),
        row=1, col=2
    )
    
    # Courbe cumulative
    sorted_grades = np.sort(grades)
    cumulative = np.arange(1, len(sorted_grades) + 1) / len(sorted_grades) * 100
    
    fig.add_trace(
        go.Scatter(x=sorted_grades, y=cumulative, mode='lines', name='Cumulative'),
        row=2, col=1
    )
    
    # Teneurs vs Distance (si disponible)
    if 'DistanceToShear' in df.columns:
        # √âchantillonner pour performance
        sample_data = df.sample(min(1000, len(df)))
        fig.add_trace(
            go.Scatter(
                x=sample_data['DistanceToShear'],
                y=pd.to_numeric(sample_data[mapping['Au']], errors='coerce'),
                mode='markers',
                marker=dict(
                    color=pd.to_numeric(sample_data[mapping['Au']], errors='coerce'),
                    colorscale='Viridis',
                    size=4,
                    opacity=0.6
                ),
                name='Au vs Distance'
            ),
            row=2, col=2
        )
    
    fig.update_layout(
        height=600,
        title_text="Analyse Statistique des Teneurs en Or",
        showlegend=False
    )
    
    return fig

def create_distance_analysis_plot(df):
    """Cr√©er une analyse de la distribution des distances"""
    
    if 'DistanceToShear' not in df.columns:
        return None
    
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=(
            'Distribution des Distances √† la Shear Zone',
            'Teneurs Moyennes par Zone de Distance',
            '√âchantillons par Zone G√©ologique',
            'Corr√©lation Distance-Teneur'
        ),
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    distances = df['DistanceToShear'].dropna()
    
    # Histogramme des distances
    fig.add_trace(
        go.Histogram(x=distances, nbinsx=30, name='Distribution Distance', opacity=0.7),
        row=1, col=1
    )
    
    # Teneurs moyennes par zones de distance
    distance_bins = pd.cut(distances, bins=10)
    grade_by_distance = df.groupby(distance_bins)['Au'].agg(['mean', 'count']).reset_index()
    grade_by_distance['distance_center'] = grade_by_distance['DistanceToShear'].apply(lambda x: x.mid)
    
    fig.add_trace(
        go.Bar(
            x=grade_by_distance['distance_center'],
            y=grade_by_distance['mean'],
            name='Teneur Moyenne',
            opacity=0.8
        ),
        row=1, col=2
    )
    
    # √âchantillons par zone g√©ologique (si disponible)
    if 'Zone' in df.columns:
        zone_counts = df['Zone'].value_counts()
        fig.add_trace(
            go.Bar(x=zone_counts.index, y=zone_counts.values, name='√âchantillons par Zone'),
            row=2, col=1
        )
    
    # Corr√©lation distance-teneur
    sample_data = df.sample(min(1000, len(df)))
    fig.add_trace(
        go.Scatter(
            x=sample_data['DistanceToShear'],
            y=sample_data['Au'],
            mode='markers',
            marker=dict(
                size=4,
                opacity=0.6,
                color=sample_data['Au'],
                colorscale='Viridis'
            ),
            name='Corr√©lation'
        ),
        row=2, col=2
    )
    
    fig.update_layout(
        height=600,
        title_text="Analyse des Distances √† la Shear Zone",
        showlegend=False
    )
    
    return fig

def create_results_analysis_plot(results_df):
    """Cr√©er un graphique d'analyse des r√©sultats"""
    
    if results_df is None or results_df.empty:
        return None
    
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=(
            'Distribution des Longueurs',
            'Distribution des Teneurs Moyennes', 
            'Grade √ó Longueur (Bubble Chart)',
            'Efficacit√© du Regroupement'
        ),
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    # Distribution des longueurs
    fig.add_trace(
        go.Histogram(x=results_df['Length'], nbinsx=20, name='Longueurs', opacity=0.7),
        row=1, col=1
    )
    
    # Distribution des teneurs
    fig.add_trace(
        go.Histogram(x=results_df['AvgGrade'], nbinsx=20, name='Teneurs', opacity=0.7),
        row=1, col=2
    )
    
    # Grade √ó Longueur
    fig.add_trace(
        go.Scatter(
            x=results_df['Length'],
            y=results_df['AvgGrade'],
            mode='markers',
            marker=dict(
                size=results_df['GradeXLength'] / 2,
                color=results_df['GradeXLength'],
                colorscale='Plasma',
                opacity=0.7,
                sizemin=4
            ),
            name='G√óL',
            text=results_df['HoleID'],
            hovertemplate='Forage: %{text}<br>Longueur: %{x:.1f}m<br>Teneur: %{y:.3f}g/t<br>G√óL: %{marker.color:.2f}<extra></extra>'
        ),
        row=2, col=1
    )
    
    # Efficacit√© du regroupement
    regrouping_data = results_df.groupby('IntervalsBefore').size().reset_index()
    regrouping_data.columns = ['IntervalsBefore', 'Count']
    
    fig.add_trace(
        go.Bar(
            x=regrouping_data['IntervalsBefore'],
            y=regrouping_data['Count'],
            name='Regroupement',
            hovertemplate='Intervalles Avant: %{x}<br>Nombre de Forages: %{y}<extra></extra>'
        ),
        row=2, col=2
    )
    
    fig.update_layout(
        height=600,
        title_text="Synth√®se des R√©sultats d'Analyse",
        showlegend=False
    )
    
    return fig

# ===============================================================================
# INTERFACE PRINCIPALE
# ===============================================================================

def main():
    # Header principal
    st.markdown("""
    <div class="main-header">
        <h1>‚õèÔ∏è Analyseur d'Intervalles Min√©ralis√©s</h1>
        <p><strong>Optimisation g√©ologique pour Leapfrog Geo | D√©velopp√© par Didier Ouedraogo, P.Geo</strong></p>
        <p><em>Contraintes de distance ET dilution - Un intervalle par forage/shear zone</em></p>
        <p><small>üìç Calcul automatique des distances perpendiculaires √† la shear zone (N45¬∞E, 75¬∞SE)</small></p>
    </div>
    """, unsafe_allow_html=True)

    # Sidebar
    with st.sidebar:
        st.image("https://via.placeholder.com/200x80/3b82f6/ffffff?text=MINERAL+ANALYZER", width=200)
        
        st.markdown("---")
        st.markdown("### üéØ Fonctionnalit√©s")
        st.markdown("""
        - ‚úÖ Import donn√©es CSV
        - ‚úÖ Mapping colonnes intelligent  
        - ‚úÖ Calcul distances automatique
        - ‚úÖ Analyse contraintes dilution
        - ‚úÖ Statistiques avanc√©es
        - ‚úÖ Export multi-format
        """)
        
        st.markdown("---")
        st.markdown("### üìê Shear Zone")
        st.markdown("""
        - **Strike:** N45¬∞E  
        - **Dip:** 75¬∞ vers SE
        - **Centre:** E450000, N5550000, Z150m
        - **Calcul:** Distance perpendiculaire
        """)
        
        st.markdown("---")
        st.markdown("### üë®‚Äçüî¨ D√©velopp√© par")
        st.markdown("**Didier Ouedraogo, P.Geo**")
        st.markdown("G√©ologue Professionnel")
        st.markdown(f"üìÖ {datetime.now().strftime('%d/%m/%Y')}")

    # Navigation par onglets
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "üìä Import & Aper√ßu", 
        "üéõÔ∏è Mapping & Config", 
        "üìà Statistiques & Graphiques", 
        "üî¨ Analyse", 
        "üìã R√©sultats & Export"
    ])

    # ========================================
    # TAB 1: IMPORT ET APER√áU
    # ========================================
    with tab1:
        st.header("üìä Import et Aper√ßu des Donn√©es")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.subheader("üéØ Options d'Import")
            
            import_option = st.radio(
                "Choisissez votre option:",
                ["üìÅ Importer fichier CSV", "üöÄ G√©n√©rer donn√©es de d√©monstration"],
                index=0
            )
            
            if import_option == "üìÅ Importer fichier CSV":
                uploaded_file = st.file_uploader(
                    "S√©lectionnez votre fichier CSV",
                    type=['csv'],
                    help="Colonnes requises: HoleID, From, To, Au. Optionnelles: Easting, Northing, Elevation"
                )
                
                if uploaded_file is not None:
                    samples_df = load_csv_file(uploaded_file)
                    if samples_df is not None:
                        st.session_state.samples_df = samples_df
                        st.session_state.drillholes_df = None
                        st.session_state.data_source = "imported"
                        st.success(f"‚úÖ {len(samples_df):,} √©chantillons import√©s")
            
            else:
                if st.button("üöÄ G√©n√©rer Donn√©es Demo", type="primary", use_container_width=True):
                    with st.spinner("G√©n√©ration des donn√©es de d√©monstration..."):
                        samples_df, drillholes_df = generate_demo_data()
                        st.session_state.samples_df = samples_df
                        st.session_state.drillholes_df = drillholes_df
                        st.session_state.data_source = "demo"
                        st.success("‚úÖ Donn√©es de d√©monstration g√©n√©r√©es!")
        
        with col2:
            st.subheader("üìã Format CSV Attendu")
            st.code("""
HoleID,From,To,Au,Easting,Northing,Elevation
DDH-001,0,2,0.15,450100,5550200,245.5
DDH-001,2,4,1.25,450102,5550198,244.2
DDH-001,4,6,0.45,450104,5550196,243.1
            """, language="csv")
            
            st.markdown("---")
            st.markdown("### ‚ÑπÔ∏è Informations")
            st.markdown("""
            **Colonnes obligatoires:**
            - HoleID, From, To, Au
            
            **Colonnes optionnelles:**
            - Easting, Northing, Elevation
            - Si pr√©sentes ‚Üí calcul distance
            - Si absentes ‚Üí distance = 999m
            """)

        # Aper√ßu des donn√©es
        if 'samples_df' in st.session_state:
            st.markdown("---")
            st.subheader("üëÄ Aper√ßu des Donn√©es")
            
            df = st.session_state.samples_df
            
            # M√©triques g√©n√©rales
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("üìä √âchantillons", f"{len(df):,}")
            
            with col2:
                st.metric("üìã Colonnes", f"{len(df.columns)}")
            
            with col3:
                unique_holes = df[df.columns[0]].nunique() if len(df) > 0 else 0
                st.metric("üó∫Ô∏è Forages", f"{unique_holes}")
            
            with col4:
                data_source = st.session_state.get('data_source', 'unknown')
                source_label = "Demo" if data_source == "demo" else "Import√©"
                st.metric("üìÅ Source", source_label)

            # Informations d√©taill√©es
            if st.session_state.get('data_source') == 'demo':
                st.markdown("""
                <div class="info-box">
                    <h4>üìä Donn√©es de D√©monstration G√©n√©r√©es</h4>
                    <ul>
                        <li><strong>100 forages</strong> optimis√©s pour intercepter la shear zone</li>
                        <li><strong>5000+ √©chantillons</strong> avec positions 3D r√©alistes</li>
                        <li><strong>Distances calcul√©es</strong> automatiquement</li>
                        <li><strong>Teneurs variables</strong> selon proximit√© shear zone</li>
                    </ul>
                </div>
                """, unsafe_allow_html=True)

            # Aper√ßu du tableau
            st.subheader("üìã Tableau des Donn√©es (50 premi√®res lignes)")
            st.dataframe(df.head(50), use_container_width=True, height=300)
            
            # Informations sur les colonnes
            with st.expander("üìä Informations D√©taill√©es sur les Colonnes"):
                col_info = pd.DataFrame({
                    'Colonne': df.columns,
                    'Type': df.dtypes.astype(str),
                    'Non-Null': df.count(),
                    'Null': df.isnull().sum(),
                    'Unique': [df[col].nunique() for col in df.columns],
                    'Exemples': [', '.join(map(str, df[col].dropna().unique()[:3])) for col in df.columns]
                })
                st.dataframe(col_info, use_container_width=True)

    # ========================================
    # TAB 2: MAPPING ET CONFIGURATION
    # ========================================
    with tab2:
        st.header("üéõÔ∏è Mapping des Colonnes et Configuration")
        
        if 'samples_df' not in st.session_state:
            st.warning("‚ö†Ô∏è Veuillez d'abord importer des donn√©es dans l'onglet 'Import & Aper√ßu'")
            return
        
        df = st.session_state.samples_df
        
        col1, col2 = st.columns([1, 1])
        
        with col1:
            st.subheader("üîß Mapping des Colonnes")
            
            # Auto-d√©tection
            if st.button("üîç Auto-d√©tection des Colonnes"):
                auto_mapping = auto_detect_columns(df)
                for key, value in auto_mapping.items():
                    st.session_state[f'mapping_{key}'] = value
                st.success("D√©tection automatique effectu√©e!")
            
            # Configuration manuelle
            available_columns = [''] + df.columns.tolist()
            
            mapping = {}
            field_definitions = {
                'HoleID': ('Identifiant Forage*', 'Identifiant unique du forage'),
                'From': ('Profondeur D√©but*', 'Profondeur de d√©but en m√®tres'),
                'To': ('Profondeur Fin*', 'Profondeur de fin en m√®tres'),
                'Au': ('Teneur Or*', 'Teneur en or (g/t)'),
                'Easting': ('Coordonn√©e Est', 'Coordonn√©e X pour calcul distance'),
                'Northing': ('Coordonn√©e Nord', 'Coordonn√©e Y pour calcul distance'),
                'Elevation': ('√âl√©vation', '√âl√©vation Z pour calcul distance')
            }
            
            for field, (label, description) in field_definitions.items():
                mapping[field] = st.selectbox(
                    f"{label}",
                    available_columns,
                    key=f'mapping_{field}',
                    help=description
                )
            
            st.session_state.column_mapping = mapping
            
            # Validation
            errors = validate_data(df, mapping)
            
            if errors:
                st.markdown('<div class="error-box">', unsafe_allow_html=True)
                st.error("‚ùå Erreurs de validation:")
                for error in errors:
                    st.write(f"‚Ä¢ {error}")
                st.markdown('</div>', unsafe_allow_html=True)
            else:
                st.markdown('<div class="success-box">', unsafe_allow_html=True)
                st.success("‚úÖ Mapping valid√© avec succ√®s!")
                st.markdown('</div>', unsafe_allow_html=True)
                
                # Calculer les distances si n√©cessaire
                if st.button("üìç Calculer Distances √† la Shear Zone"):
                    df_with_distances, has_coords = calculate_distances_if_needed(df, mapping)
                    st.session_state.samples_df = df_with_distances
                    st.session_state.has_coordinates = has_coords

        with col2:
            st.subheader("‚öôÔ∏è Configuration des Param√®tres")
            
            # Param√®tres principaux
            config = {}
            config['cutoff_grade'] = st.slider(
                "üíé Teneur de Coupure (g/t Au)",
                min_value=0.1, max_value=5.0, value=0.5, step=0.1,
                help="Teneur minimum pour inclusion"
            )
            
            config['min_length'] = st.slider(
                "üìè Longueur Minimale (m)",
                min_value=0.5, max_value=20.0, value=2.0, step=0.5,
                help="Longueur minimum de l'intervalle"
            )
            
            config['min_samples'] = st.slider(
                "üî¢ √âchantillons Minimum",
                min_value=1, max_value=10, value=3,
                help="Nombre minimum d'√©chantillons"
            )
            
            st.markdown("#### üî¥ Contraintes Avanc√©es")
            
            config['max_distance'] = st.slider(
                "‚≠ê Distance Max Shear Zone (m)",
                min_value=1.0, max_value=50.0, value=10.0, step=1.0,
                help="Distance maximum √† la shear zone"
            )
            
            config['max_dilution'] = st.slider(
                "üî• Dilution Maximale (m)",
                min_value=1.0, max_value=50.0, value=10.0, step=1.0,
                help="Dilution maximum permise"
            )
            
            st.session_state.analysis_config = config
            
            # R√©sum√© de la configuration
            st.markdown("#### üìã R√©sum√© Configuration")
            config_display = {
                "Teneur de coupure": f"{config['cutoff_grade']} g/t",
                "Longueur minimale": f"{config['min_length']} m",
                "√âchantillons minimum": f"{config['min_samples']}",
                "Distance max shear": f"{config['max_distance']} m",
                "Dilution maximale": f"{config['max_dilution']} m"
            }
            
            for key, value in config_display.items():
                st.write(f"**{key}:** {value}")

    # ========================================
    # TAB 3: STATISTIQUES ET GRAPHIQUES
    # ========================================
    with tab3:
        st.header("üìà Statistiques et Graphiques")
        
        if 'samples_df' not in st.session_state:
            st.warning("‚ö†Ô∏è Veuillez d'abord importer des donn√©es")
            return
        
        if 'column_mapping' not in st.session_state:
            st.warning("‚ö†Ô∏è Veuillez d'abord configurer le mapping des colonnes")
            return
        
        df = st.session_state.samples_df
        mapping = st.session_state.column_mapping
        
        # Statistiques g√©n√©rales
        st.subheader("üìä Statistiques G√©n√©rales")
        
        if mapping.get('Au'):
            grades = pd.to_numeric(df[mapping['Au']], errors='coerce').dropna()
            
            col1, col2, col3, col4, col5, col6 = st.columns(6)
            
            with col1:
                st.metric("üìä √âchantillons", f"{len(grades):,}")
            
            with col2:
                st.metric("üí∞ Teneur Min", f"{grades.min():.3f} g/t")
            
            with col3:
                st.metric("üìà Teneur Moy", f"{grades.mean():.3f} g/t")
            
            with col4:
                st.metric("üìä M√©diane", f"{grades.median():.3f} g/t")
            
            with col5:
                st.metric("üî∫ P95", f"{grades.quantile(0.95):.3f} g/t")
            
            with col6:
                st.metric("üéØ Teneur Max", f"{grades.max():.3f} g/t")
            
            # Statistiques par seuils
            st.markdown("---")
            st.subheader("üéØ Statistiques par Seuils de Teneur")
            
            thresholds = [0.5, 1.0, 2.0, 5.0]
            threshold_stats = []
            
            for threshold in thresholds:
                count = len(grades[grades >= threshold])
                percentage = (count / len(grades)) * 100 if len(grades) > 0 else 0
                threshold_stats.append({
                    'Seuil (g/t)': f"‚â• {threshold}",
                    '√âchantillons': count,
                    'Pourcentage': f"{percentage:.1f}%"
                })
            
            threshold_df = pd.DataFrame(threshold_stats)
            st.dataframe(threshold_df, use_container_width=True)
        
        # Graphiques de distribution
        st.markdown("---")
        st.subheader("üìà Analyse Graphique des Teneurs")
        
        fig_dist = create_grade_distribution_plot(df, mapping)
        if fig_dist:
            st.plotly_chart(fig_dist, use_container_width=True)
        
        # Analyse des distances (si disponible)
        if 'DistanceToShear' in df.columns:
            st.markdown("---")
            st.subheader("üìç Analyse des Distances √† la Shear Zone")
            
            distances = df['DistanceToShear'].dropna()
            
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.metric("üìç Distance Min", f"{distances.min():.1f} m")
            
            with col2:
                st.metric("üìä Distance Moy", f"{distances.mean():.1f} m")
            
            with col3:
                st.metric("üìà M√©diane", f"{distances.median():.1f} m")
            
            with col4:
                st.metric("üéØ Distance Max", f"{distances.max():.1f} m")
            
            # Statistiques par zones de distance
            st.markdown("#### üè∑Ô∏è R√©partition par Zones de Distance")
            
            zone_stats = []
            zones = [
                ("Tr√®s proche", 0, 5),
                ("Proche", 5, 15),
                ("Mod√©r√©", 15, 30),
                ("√âloign√©", 30, float('inf'))
            ]
            
            for zone_name, min_dist, max_dist in zones:
                if max_dist == float('inf'):
                    zone_samples = distances[distances >= min_dist]
                else:
                    zone_samples = distances[(distances >= min_dist) & (distances < max_dist)]
                
                count = len(zone_samples)
                percentage = (count / len(distances)) * 100 if len(distances) > 0 else 0
                
                # Teneur moyenne dans cette zone
                if mapping.get('Au'):
                    zone_mask = df['DistanceToShear'].between(min_dist, max_dist if max_dist != float('inf') else df['DistanceToShear'].max())
                    zone_grades = pd.to_numeric(df[zone_mask][mapping['Au']], errors='coerce').dropna()
                    avg_grade = zone_grades.mean() if len(zone_grades) > 0 else 0
                else:
                    avg_grade = 0
                
                zone_stats.append({
                    'Zone': zone_name,
                    'Distance (m)': f"{min_dist}-{max_dist if max_dist != float('inf') else '‚àû'}",
                    '√âchantillons': count,
                    'Pourcentage': f"{percentage:.1f}%",
                    'Teneur Moy (g/t)': f"{avg_grade:.3f}"
                })
            
            zone_df = pd.DataFrame(zone_stats)
            st.dataframe(zone_df, use_container_width=True)
            
            # Graphique des distances
            fig_distance = create_distance_analysis_plot(df)
            if fig_distance:
                st.plotly_chart(fig_distance, use_container_width=True)

    # ========================================
    # TAB 4: ANALYSE
    # ========================================
    with tab4:
        st.header("üî¨ Analyse des Intervalles Min√©ralis√©s")
        
        if 'samples_df' not in st.session_state:
            st.warning("‚ö†Ô∏è Veuillez d'abord importer des donn√©es")
            return
        
        if 'column_mapping' not in st.session_state or 'analysis_config' not in st.session_state:
            st.warning("‚ö†Ô∏è Veuillez d'abord configurer le mapping et les param√®tres")
            return
        
        df = st.session_state.samples_df
        mapping = st.session_state.column_mapping
        config = st.session_state.analysis_config
        
        # V√©rifier la validation
        errors = validate_data(df, mapping)
        if errors:
            st.error("‚ùå Erreurs de validation. Veuillez corriger le mapping.")
            for error in errors:
                st.write(f"‚Ä¢ {error}")
            return
        
        # Afficher la configuration active
        st.subheader("üìã Configuration Active")
        
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("üíé Teneur Coupure", f"{config['cutoff_grade']} g/t")
        with col2:
            st.metric("üìè Long. Min", f"{config['min_length']} m")
        with col3:
            st.metric("üî¢ √âch. Min", f"{config['min_samples']}")
        with col4:
            st.metric("‚≠ê Dist. Max", f"{config['max_distance']} m")
        with col5:
            st.metric("üî• Dilution Max", f"{config['max_dilution']} m")
        
        # Statistiques pr√©-analyse
        st.subheader("üìä Statistiques Pr√©-Analyse")
        
        # Pr√©parer les donn√©es d'analyse
        analysis_df = df.copy()
        
        # S'assurer que les distances sont calcul√©es
        if 'DistanceToShear' not in analysis_df.columns:
            if all(mapping.get(field) for field in ['Easting', 'Northing', 'Elevation']):
                analysis_df, _ = calculate_distances_if_needed(analysis_df, mapping)
                st.session_state.samples_df = analysis_df
            else:
                analysis_df['DistanceToShear'] = 999
        
        # Renommer les colonnes selon le mapping
        column_rename = {v: k for k, v in mapping.items() if v}
        analysis_df = analysis_df.rename(columns=column_rename)
        
        # Convertir en num√©rique
        numeric_cols = ['From', 'To', 'Au']
        for col in numeric_cols:
            if col in analysis_df.columns:
                analysis_df[col] = pd.to_numeric(analysis_df[col], errors='coerce')
        
        if 'Au' in analysis_df.columns:
            grades = analysis_df['Au'].dropna()
            above_cutoff = len(grades[grades >= config['cutoff_grade']])
            
            if 'DistanceToShear' in analysis_df.columns:
                close_to_shear = len(analysis_df[analysis_df['DistanceToShear'] <= config['max_distance']])
                valid_samples = len(analysis_df[
                    (analysis_df['Au'] >= config['cutoff_grade']) & 
                    (analysis_df['DistanceToShear'] <= config['max_distance'])
                ])
            else:
                close_to_shear = len(analysis_df)
                valid_samples = above_cutoff
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üìà Total √âchantillons", f"{len(grades):,}")
            with col2:
                st.metric("üí∞ > Teneur Coupure", f"{above_cutoff:,}")
            with col3:
                st.metric("üìç Proches Shear Zone", f"{close_to_shear:,}")
            with col4:
                st.metric("‚úÖ Crit√®res Combin√©s", f"{valid_samples:,}")
        
        # Bouton d'analyse
        st.markdown("---")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            if st.button("üöÄ Lancer l'Analyse Compl√®te", type="primary", use_container_width=True):
                with st.spinner("Analyse en cours..."):
                    try:
                        # Barre de progression
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        
                        status_text.text("üìä Pr√©paration des donn√©es...")
                        progress_bar.progress(20)
                        
                        status_text.text("üîç Identification des intervalles potentiels...")
                        progress_bar.progress(40)
                        
                        # Calculer les intervalles potentiels
                        potential_intervals = calculate_mineral_intervals(analysis_df, config)
                        
                        status_text.text("üî• Application des contraintes de dilution...")
                        progress_bar.progress(60)
                        
                        # Appliquer les contraintes de dilution
                        final_intervals = apply_dilution_constraints(
                            potential_intervals, 
                            analysis_df, 
                            config['max_dilution']
                        )
                        
                        status_text.text("‚úÖ Finalisation des r√©sultats...")
                        progress_bar.progress(80)
                        
                        # Validation finale
                        validated_intervals = final_intervals[
                            (final_intervals['Length'] >= config['min_length']) &
                            (final_intervals['AvgGrade'] >= config['cutoff_grade']) &
                            (final_intervals['SampleCount'] >= config['min_samples'])
                        ].copy()
                        
                        # R√©indexer
                        validated_intervals.reset_index(drop=True, inplace=True)
                        validated_intervals['IntervalID'] = range(1, len(validated_intervals) + 1)
                        
                        st.session_state.analysis_results = validated_intervals
                        
                        progress_bar.progress(100)
                        status_text.text("üéâ Analyse termin√©e!")
                        
                        # Nettoyer l'interface
                        import time
                        time.sleep(1)
                        progress_bar.empty()
                        status_text.empty()
                        
                        if len(validated_intervals) > 0:
                            st.success(f"‚úÖ Analyse termin√©e! {len(validated_intervals)} intervalles trouv√©s.")
                        else:
                            st.warning("‚ö†Ô∏è Aucun intervalle ne respecte les contraintes d√©finies.")
                            
                    except Exception as e:
                        st.error(f"‚ùå Erreur lors de l'analyse: {str(e)}")
                        progress_bar.empty()
                        status_text.empty()

        # Afficher les r√©sultats si disponibles
        if 'analysis_results' in st.session_state:
            results_df = st.session_state.analysis_results
            
            if not results_df.empty:
                st.markdown("---")
                st.subheader("üéâ R√©sultats de l'Analyse")
                
                # M√©triques de synth√®se
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("üéØ Intervalles", f"{len(results_df)}")
                
                with col2:
                    avg_grade = results_df['AvgGrade'].mean()
                    st.metric("üìä Teneur Moy.", f"{avg_grade:.3f} g/t")
                
                with col3:
                    total_length = results_df['Length'].sum()
                    st.metric("üìè Long. Totale", f"{total_length:.1f} m")
                
                with col4:
                    total_metal = results_df['GradeXLength'].sum()
                    st.metric("üí∞ Grade√óLong.", f"{total_metal:.1f}")
                
                # Tableau des r√©sultats
                st.subheader("üìã D√©tail des Intervalles")
                st.dataframe(
                    results_df.round(3),
                    use_container_width=True,
                    height=300
                )
                
                # Graphique d'analyse des r√©sultats
                fig_results = create_results_analysis_plot(results_df)
                if fig_results:
                    st.plotly_chart(fig_results, use_container_width=True)

    # ========================================
    # TAB 5: R√âSULTATS ET EXPORT
    # ========================================
    with tab5:
        st.header("üìã R√©sultats et Export")
        
        if 'analysis_results' not in st.session_state:
            st.warning("‚ö†Ô∏è Aucune analyse effectu√©e. Veuillez lancer l'analyse dans l'onglet pr√©c√©dent.")
            return
        
        results_df = st.session_state.analysis_results
        
        if results_df.empty:
            st.warning("‚ö†Ô∏è Aucun intervalle trouv√© avec les crit√®res actuels.")
            return
        
        # M√©triques principales
        st.subheader("üìä M√©triques Principales")
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric(
                "üéØ Intervalles Totaux",
                f"{len(results_df)}",
                delta="Un par forage"
            )
        
        with col2:
            avg_grade = results_df['AvgGrade'].mean()
            max_grade = results_df['MaxGrade'].max()
            st.metric(
                "üìä Teneur Moyenne",
                f"{avg_grade:.3f} g/t",
                delta=f"Max: {max_grade:.3f}"
            )
        
        with col3:
            total_length = results_df['Length'].sum()
            avg_length = results_df['Length'].mean()
            st.metric(
                "üìè Longueur Totale",
                f"{total_length:.1f} m",
                delta=f"Moy: {avg_length:.1f}m"
            )
        
        with col4:
            total_metal = results_df['GradeXLength'].sum()
            st.metric(
                "üí∞ M√©tal Total (G√óL)",
                f"{total_metal:.1f}",
                delta="Potentiel √©conomique"
            )
        
        # Options d'export
        st.markdown("---")
        st.subheader("üíæ Options d'Export")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # Export CSV standard
            csv_data = results_df.to_csv(index=False)
            st.download_button(
                label="üìä T√©l√©charger CSV",
                data=csv_data,
                file_name=f"intervalles_mineralises_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                mime="text/csv",
                use_container_width=True
            )
        
        with col2:
            # Export Excel avec m√©tadonn√©es
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                results_df.to_excel(writer, sheet_name='Intervalles', index=False)
                
                # Feuille m√©tadonn√©es
                if 'analysis_config' in st.session_state:
                    config = st.session_state.analysis_config
                    metadata = pd.DataFrame({
                        'Param√®tre': [
                            'Date Analyse',
                            'Teneur Coupure (g/t)',
                            'Longueur Min (m)',
                            '√âchantillons Min',
                            'Distance Max (m)',
                            'Dilution Max (m)',
                            'Intervalles Trouv√©s',
                            'Shear Zone Strike',
                            'Shear Zone Dip'
                        ],
                        'Valeur': [
                            datetime.now().strftime('%Y-%m-%d %H:%M'),
                            config['cutoff_grade'],
                            config['min_length'],
                            config['min_samples'],
                            config['max_distance'],
                            config['max_dilution'],
                            len(results_df),
                            'N45¬∞E',
                            '75¬∞ SE'
                        ]
                    })
                    metadata.to_excel(writer, sheet_name='M√©tadonn√©es', index=False)
            
            st.download_button(
                label="üìã T√©l√©charger Excel",
                data=buffer.getvalue(),
                file_name=f"intervalles_mineralises_{datetime.now().strftime('%Y%m%d_%H%M')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )
        
        with col3:
            # Export format Leapfrog
            leapfrog_data = results_df.copy()
            leapfrog_data['CompositeID'] = leapfrog_data['IntervalID']
            leapfrog_data['Grade'] = leapfrog_data['AvgGrade']
            leapfrog_data['Thickness'] = leapfrog_data['Length']
            leapfrog_data['Quality'] = 'High'
            leapfrog_data['AnalysisDate'] = datetime.now().strftime('%Y-%m-%d')
            leapfrog_data['Method'] = 'Dilution_Constrained'
            
            leapfrog_csv = leapfrog_data[[
                'CompositeID', 'HoleID', 'From', 'To', 'Thickness', 
                'Grade', 'Quality', 'AnalysisDate', 'Method'
            ]].to_csv(index=False)
            
            st.download_button(
                label="üó∫Ô∏è Export Leapfrog",
                data=leapfrog_csv,
                file_name=f"leapfrog_intervals_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                mime="text/csv",
                use_container_width=True
            )
        
        # Tableau d√©taill√© final avec filtres
        st.markdown("---")
        st.subheader("üìã Tableau D√©taill√© avec Filtres")
        
        # Filtres
        col1, col2, col3 = st.columns(3)
        
        with col1:
            min_grade_filter = st.slider(
                "Teneur minimum:",
                min_value=0.0,
                max_value=float(results_df['AvgGrade'].max()),
                value=0.0,
                step=0.1
            )
        
        with col2:
            min_length_filter = st.slider(
                "Longueur minimum:",
                min_value=0.0,
                max_value=float(results_df['Length'].max()),
                value=0.0,
                step=0.5
            )
        
        with col3:
            holes_filter = st.multiselect(
                "Forages √† afficher:",
                options=results_df['HoleID'].unique(),
                default=results_df['HoleID'].unique()
            )
        
        # Appliquer les filtres
        filtered_df = results_df[
            (results_df['AvgGrade'] >= min_grade_filter) &
            (results_df['Length'] >= min_length_filter) &
            (results_df['HoleID'].isin(holes_filter))
        ]
        
        st.dataframe(filtered_df.round(3), use_container_width=True, height=400)
        
        # Statistiques finales
        st.markdown("---")
        st.subheader("üìä Statistiques Finales")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üéØ Distribution par Qualit√©")
            
            excellent = len(filtered_df[filtered_df['GradeXLength'] >= 10])
            bon = len(filtered_df[(filtered_df['GradeXLength'] >= 5) & (filtered_df['GradeXLength'] < 10)])
            modere = len(filtered_df[filtered_df['GradeXLength'] < 5])
            
            quality_data = pd.DataFrame({
                'Qualit√©': ['Excellent (G√óL‚â•10)', 'Bon (5‚â§G√óL<10)', 'Mod√©r√© (G√óL<5)'],
                'Nombre': [excellent, bon, modere],
                'Pourcentage': [
                    f"{excellent/len(filtered_df)*100:.1f}%" if len(filtered_df) > 0 else "0%",
                    f"{bon/len(filtered_df)*100:.1f}%" if len(filtered_df) > 0 else "0%",
                    f"{modere/len(filtered_df)*100:.1f}%" if len(filtered_df) > 0 else "0%"
                ]
            })
            
            st.dataframe(quality_data, use_container_width=True)
        
        with col2:
            st.markdown("#### üìè Distribution par Longueur")
            
            court = len(filtered_df[filtered_df['Length'] < 5])
            moyen = len(filtered_df[(filtered_df['Length'] >= 5) & (filtered_df['Length'] < 10)])
            long = len(filtered_df[filtered_df['Length'] >= 10])
            
            length_data = pd.DataFrame({
                'Longueur': ['Court (<5m)', 'Moyen (5-10m)', 'Long (‚â•10m)'],
                'Nombre': [court, moyen, long],
                'Pourcentage': [
                    f"{court/len(filtered_df)*100:.1f}%" if len(filtered_df) > 0 else "0%",
                    f"{moyen/len(filtered_df)*100:.1f}%" if len(filtered_df) > 0 else "0%",
                    f"{long/len(filtered_df)*100:.1f}%" if len(filtered_df) > 0 else "0%"
                ]
            })
            
            st.dataframe(length_data, use_container_width=True)
        
        # Information sur l'efficacit√© du regroupement
        if 'IntervalsBefore' in results_df.columns:
            st.markdown("---")
            st.subheader("üîÑ Efficacit√© du Regroupement")
            
            regrouped = len(results_df[results_df['IntervalsBefore'] > 1])
            single = len(results_df[results_df['IntervalsBefore'] == 1])
            
            st.markdown(f"""
            <div class="info-box">
                <h4>üìä R√©sum√© du Regroupement</h4>
                <ul>
                    <li><strong>{single} forages</strong> avec un seul intervalle</li>
                    <li><strong>{regrouped} forages</strong> avec intervalles regroup√©s</li>
                    <li><strong>Taux de regroupement:</strong> {regrouped/len(results_df)*100:.1f}%</li>
                    <li><strong>R√®gle appliqu√©e:</strong> Un intervalle par forage/shear zone</li>
                </ul>
            </div>
            """, unsafe_allow_html=True)

if __name__ == "__main__":
    main()